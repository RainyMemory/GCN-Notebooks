{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Using geometric to create GCN models\n",
    "\n",
    "- The content of the note mainly comes from Stanford CS224W \n",
    "\n",
    "## Prepare for the running environment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specific CUDA version (cpu, cu92, cu101, cu102, cu110) and PyTorch version (1.4.0, 1.5.0, 1.6.0, 1.7.0)\n",
    "# !pip install --no-index torch-scatter -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.html\n",
    "# !pip install --no-index torch-sparse -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.html\n",
    "# !pip install --no-index torch-cluster -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.html\n",
    "# !pip install --no-index torch-spline-conv -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.html\n",
    "!pip install --no-index torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+cpu.html\n",
    "!pip install --no-index torch-sparse -f https://pytorch-geometric.com/whl/torch-1.7.0+cpu.html\n",
    "!pip install --no-index torch-cluster -f https://pytorch-geometric.com/whl/torch-1.7.0+cpu.html\n",
    "!pip install --no-index torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.7.0+cpu.html\n",
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.utils as pyg_utils\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import time \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from datetime import datetime\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "batch_size = 64\n",
    "hidden_dim = 64\n",
    "lr = 0.02\n",
    "epoch = 200\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "source": [
    "## Create a model with GCN layers\n",
    "\n",
    "- GCN cares about node embedding learning and graph embedding learning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNStack(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, task='node_embed', dropout_rate=0.25):\n",
    "        super(GNNStack, self).__init__()\n",
    "        self.task = task\n",
    "        if not (self.task == 'node_embed' or self.task == 'graph_embed'):\n",
    "            raise RuntimeError('Task type not supported, expect node_embed or graph_embed but got: ', self.task)\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.conv_layers.append(self.build_conv_model(input_dim, hidden_dim))\n",
    "        for _ in range(2):\n",
    "            self.conv_layers.append(self.build_conv_model(hidden_dim, hidden_dim))\n",
    "        self.post_mp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        self.layer_num = 3\n",
    "        \n",
    "    def build_conv_model(self, input_dim, hidden_dim):\n",
    "        if self.task == 'node_embed':\n",
    "            return pyg_nn.GCNConv(input_dim, hidden_dim)\n",
    "        else: # graph embedding\n",
    "            return pyt_nn.GINConv(nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.ReLu(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            ))\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        if data.num_node_features == 0:\n",
    "            x = torch.ones(data.num_nodes, 1) # node feature init (constant)\n",
    "        for layer_index in range(self.layer_num):\n",
    "            x = self.conv_layers[layer_index](x, edge_index)\n",
    "            embed = x\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "        if self.task == 'graph_embed':\n",
    "            x = pyg_nn.global_mean_pool(x, batch) # pooling all nodes together to represent whole graph\n",
    "        x = self.post_mp(x)\n",
    "        return embed, F.log_softmax(x)\n",
    "    \n",
    "    def loss(self, prediction, label):\n",
    "        return F.nll_loss(prediction, label) # negative log-likelihood (-LL), notice we use softmax for the prediciton"
   ]
  },
  {
   "source": [
    "## How to make customized GCN conv layers\n",
    "\n",
    "- The most important questions are: <br>\n",
    "    - how to define the message passing method (to aggregate information from neighbours)<br>\n",
    "    - how to define the nodes which should be the neighbours"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomizedConv(pyg_nn.MessagePassing):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(CustomizedConv, self).__init__(aggr='add') # the aggregation method\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        # self.self_linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        # x shape [Num(batch size), input_dim]\n",
    "        # edge_index shape [2, EdegNum] (from witch node to witch node)\n",
    "        # add self-loop to adjacency matrix, nodes can point to themselves (A + I)\n",
    "        edge_index, _ = pyg_utils.add_self_loop(edge_index, num_nodes=x.size(0))\n",
    "        # transform into feature matrix (run through the customized covn layers)\n",
    "        x = self.linear(x)\n",
    "        # current graph: n by n matrix, propagate will call the 'message' function\n",
    "        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x)\n",
    "        # we can also do the self information mapping independently, this depends on how u think\n",
    "        # thus we do not need the self loops, the self information will go in another way\n",
    "        # edge_index, _ = pyg_utils.remove_self_loop(edge_index)\n",
    "        # x_self = self.self_linear(x)\n",
    "        # x = self.linear(x)\n",
    "        # combine the self info into the final return\n",
    "        # return x_self + self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x)\n",
    "\n",
    "    # x_i self embedding, x_j neighbour embedding\n",
    "    def message(self, x_j, edge_index, size):\n",
    "        # compute the message flow in the graph\n",
    "        # x_j shape [Neighbour, ouput_dim]\n",
    "        # edge start nodes and end nodes (the end node is the current targe node) \n",
    "        row, col = edge_index\n",
    "        degree = pyg_util.degree(row, size[0], dtype=x_j.dtype)\n",
    "        # get the D^(-1/2) to normalise the ouput information\n",
    "        degree_inv_sqrt = degree.pow(-0.5)\n",
    "        norm  = degree_inv_sqrt[row] * degree_inv_sqrt[col]\n",
    "        return norm.view(-1, 1) * x_j\n",
    "    \n",
    "    def update(self, aggr_out):\n",
    "        # aggr_out shape [Num, ouput_dim]\n",
    "        # the aggregate layers can be defined here (after message passing)\n",
    "        return aggr_out"
   ]
  },
  {
   "source": [
    "## Define the training process"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, task, writer):\n",
    "    if task == 'graph_embed':\n",
    "        data_size = len(dataset)\n",
    "        trainloader = DataLoader(dataset[:int(data_size * 0.8)], batch_size=batch_size, shuffle=True)\n",
    "        testloader = DataLoader(dataset[int(data_size * 0.8):], batch_size=batch_size, shuffle=True)\n",
    "    else:\n",
    "        # node embedding need the information of the whole graph to do the message flow\n",
    "        trainloader = testloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    # if the nodes have no features, we init it with a constant thus it should be 1\n",
    "    model = GNNStack(input_dim=max(dataset.num_node_features, 1), hidden_dim=hidden_dim, output_dim=dataset.num_classes, task=task)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    for step in range(epoch):\n",
    "        tot_loss = 0\n",
    "        for batch in trainloader:\n",
    "            embed, prediction = model(batch)\n",
    "            label = batch.y\n",
    "            if task == 'node_embed':\n",
    "                # this just like the masks in bert\n",
    "                prediction = prediction[batch.train_mask]\n",
    "                label = label[batch.train_mask]\n",
    "            loss = model.loss(prediction, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tot_loss += loss.item() * batch.num_graphs\n",
    "        tot_loss /= len(trainloader.dataset)\n",
    "        writer.add_scalar(\"loss\", tot_loss, step)\n",
    "        if step%10 == 0:\n",
    "            test_acc = test(testloader, model)\n",
    "            print(\"Current setp {}, loss {:.4f} with test accuracy {:4f}\".format(step, tot_loss, test_acc))\n",
    "            writer.add_scalar(\"test_acc\", test_acc, step)\n",
    "    return model\n",
    "\n",
    "def test(loader, model, is_validation=False):\n",
    "    model.eval()\n",
    "    correct_num = 0\n",
    "    for data in loader:\n",
    "        # stop compute the gradient during propagation\n",
    "        with torch.no_grad():\n",
    "            embed, prediction = model(data)\n",
    "            prediction = prediction.argmax(dim=1)\n",
    "            label = data.y\n",
    "        if model.task == \"node_embed\":\n",
    "            mask = data.val_mask if is_validation else data.test_mask\n",
    "            prediction = prediction[mask]\n",
    "            label = label[mask]\n",
    "        correct_num += prediction.eq(label).sum().item()\n",
    "    tot = len(loader.dataset) if model.task is 'graph_embed' else 0\n",
    "    if tot is 0:\n",
    "        for data in loader.dataset:\n",
    "            tot += torch.sum(data.test_mask).item()\n",
    "    return correct_num / tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().system_raw(\n",
    "    'temsorboard --logdir {} --host 0.0.0.0 --port 6006 &'.format('./log')\n",
    ")\n",
    "get_ipython().system_raw(\n",
    "    './ngrok http 6006 &'\n",
    ")\n",
    "!curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print(json.load(sys,stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "source": [
    "## Load some sample datasets and try out the GCN model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"./log/\" + datetime.now().strftime(\"%Y%m%d\"))\n",
    "dataset = TUDataset(root='/tmp/IMDB-BINARY', name='IMDB-BINARY')\n",
    "dataset = dataset.shuffle()\n",
    "task = 'graph_embed'\n",
    "model = train(dataset, task, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"./log/\" + datetime.now().strftime(\"%Y%m%d\"))\n",
    "dataset = Planetoid(root='/tmp/citeseer', name='citeseer')\n",
    "task = 'node_embed'\n",
    "model = train(dataset, task, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = ['red', 'orange', 'green', 'blue', 'purple', 'black']\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "embeddings = []\n",
    "colors = []\n",
    "model.eval()\n",
    "for batch in loader:\n",
    "    embed, prediction = model(batch)\n",
    "    embeddings.append(embed)\n",
    "    colors += [color[y] for y in batch.y]\n",
    "embeddings = torch.cat(embeddings, dim=0)\n",
    "xs, ys = zip(*TSNE().fit_transform(embeddings.detach().numpy()))\n",
    "plt.scatter(xs, ys, color=colors)"
   ]
  }
 ]
}